# xpath语法

------

## 一、前言

​	在前面的文章当中，已经教大家如何去获取我们需要的数据原文内容，今天就介绍一个用于提取所需数据的方法之一xpath。在后续会讲解bs4(beautifulsoup),re正则表达式。



## 二、正文

> XPath 使用路径表达式来选取HTML/ XML 文档中的节点或节点集。节点是通过沿着路径 (path) 或者步 (steps) 来选取的。

使用到python中的一个lxml库：下载 `pip install lxml`

## 选取节点

|  表达式  | 描述                                                         |
| :------: | ------------------------------------------------------------ |
| nodename | 选取此节点的所有子节点。                                     |
|    /     | 从根节点选取（取子节点）。                                   |
|    //    | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置（取子孙节点）。 |
|    .     | 选取当前节点。                                               |
|    ..    | 选取当前节点的父节点。                                       |
|    @     | 选取属性。                                                   |



### 路径表达式

|   路径表达式    | 结果                                                         |
| :-------------: | ------------------------------------------------------------ |
|    bookstore    | 选取 bookstore 元素的所有子节点。                            |
|   /bookstore    | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |
| bookstore/book  | 选取属于 bookstore 的子元素的所有 book 元素。                |
|     //book      | 选取所有 book 子元素，而不管它们在文档中的位置。             |
| bookstore//book | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |
|     //@lang     | 选取名为 lang 的所有属性。                                   |



### 谓语

> 谓语用来查找某个特定的节点或者包含某个指定的值的节点。
>
> 谓语被嵌在方括号中。

|          路径表达式           | 结果                                                       |
| :---------------------------: | ---------------------------------------------------------- |
|      /bookstore/book[1]       | 选取属于 bookstore 子元素的第一个 book 元素。              |
|    /bookstore/book[last()]    | 选取属于 bookstore 子元素的最后一个 book 元素。            |
|   /bookstore/book[last()-1]   | 选取属于 bookstore 子元素的倒数第二个 book 元素。          |
| /bookstore/book[position()<3] | 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。  |
|        //title[@lang]         | 选取所有拥有名为 lang 的属性的 title 元素。                |
|     //title[@lang='eng']      | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 |

------



#### 选取未知节点

| 通配符 | 描述                 |
| :----: | -------------------- |
|   *    | 匹配任何元素节点。   |
|   @*   | 匹配任何属性节点。   |
| node() | 匹配任何类型的节点。 |

​	---  在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

  

|  路径表达式  | 结果                              |
| :----------: | --------------------------------- |
| /bookstore/* | 选取 bookstore 元素的所有子元素。 |
|     //*      | 选取文档中的所有元素。            |
| //title[@*]  | 选取所有带有属性的 title 元素。   |

------

#### 选取若干节点

> 通过在路径表达式中使用"|"运算符，您可以选取若干个路径。

|            路径表达式            | 结果                                                         |
| :------------------------------: | ------------------------------------------------------------ |
|   //book/title \| //book/price   | 选取 book 元素的所有 title 和 price 元素。                   |
|        //title \| //price        | 选取文档中的所有 title 和 price 元素。                       |
| /bookstore/book/title \| //price | 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 |



### 三、示例

**下面给出一个示例代码**

```python
# -*- coding:utf-8 -*-
import requests
from lxml import etree


class DouGuo(object):
    def __init__(self):
        self.url = "https://www.douguo.com/caipu/%E5%AE%B6%E5%B8%B8%E8%8F%9C/0/20"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
        }

    def get_data_index(self):
        response = requests.get(self.url, headers=self.headers)
        response.encoding="utf-8"
        if response.status_code == 200:
            return response.text
        else:
            return None

    def parse_data_index(self, response):
        html = etree.HTML(response)
        data_list = html.xpath('//ul[@class="cook-list"]//li[@class="clearfix"]')
        for data in data_list:
            # 提取文本值
            title = data.xpath("./div/a/text()")[0]
            major = data.xpath("./div/p/text()")[0]
            # 提取属性值
            head = data.xpath("./div/div[2]/a/img/@alt")[0]
            score = data.xpath("./div/div[1]//span/text()")[0]
            print(f"title: {title}\nmajor: {major}\nhead:{head}\nscore:{score}\n\n")

    def run(self):
        response = self.get_data_index()
        # print(response)
        self.parse_data_index(response)

if __name__ == '__main__':
    spider = DouGuo()
    spider.run()
```

# 四、结语

大家可以尝试去抓取这个url  `https://cs.lianjia.com/ershoufang/`

获取`第一页`数据即可，同时也可以思考一下，如何进行多页的获取，实现翻页功能。

包含的字段有：`标题，位置，户型，总价，单价`

下一篇文章会讲解如何去抓取 `安居客`中的 `房价 地段` 等内容。`https://cs.anjuke.com/sale/yuelu/`

<hr>

​	有任何问题欢迎大家的评论和指正。再次声明，本专栏只做技术探讨，严谨商用，恶意攻击等。

这是我的 GitHub 主页：[Rosyrain (github.com)](https://github.com/Rosyrain)  `https://github.com/rosyrain`，里面有一些我学习时候的笔记或者代码。本专栏的文档和源码存到spider-course的仓库下。

欢迎大家**Follow/Star/Fork**三连。