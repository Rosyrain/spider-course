# BeautifulSoup解析

## 一、前言

在前面两篇文章，我们讲解了如何使用xpath解析去提取数据。本文将继续讲解另一种提取方法---**BeautifulSoup**，也叫做bs4解析。

## 二、正文

> ​	**下载 -- pip install bs4**

**示例代码-爱丽丝漫游仙境**

~~~python
html_doc = """
<html><head><title>The Dormouse's story</title></head>
    <body>
        <p class="title"><b>The Dormouse's story</b></p>
        <p class="story">Once upon a time there were three little sisters; and their names were
            <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
            <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
            <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
            and they lived at the bottom of a well.
        </p>

<p class="story">...</p>
"""
~~~

使用bs4格式化输出代码

~~~ python
from bs4 import BeautifulSoup
# lxml为解析器
soup = BeautifulSoup(html_doc,"lxml")
# 格式化输出代码
print(soup.prettify())
~~~

------



| 解析器           | 使用方法                                                     | 优势                                                  | 劣势                                            |
| ---------------- | ------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------- |
| Python标准库     | `BeautifulSoup(markup, "html.parser")`                       | Python的内置标准库执行速度适中文档容错能力强          | Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差 |
| lxml HTML 解析器 | `BeautifulSoup(markup, "lxml")`                              | 速度快文档容错能力强                                  | 需要安装C语言库                                 |
| lxml XML 解析器  | `BeautifulSoup(markup, ["lxml-xml"])``BeautifulSoup(markup, "xml")` | 速度快唯一支持XML的解析器                             | 需要安装C语言库                                 |
| html5lib         | `BeautifulSoup(markup, "html5lib")`                          | 最好的容错性以浏览器的方式解析文档生成HTML5格式的文档 | 速度慢不依赖外部扩展                            |

推荐使用lxml作为解析器,因为效率更高. 在Python2.7.3之前的版本和Python3中3.2.2之前的版本,必须安装lxml或html5lib, 因为那些Python版本的标准库中内置的HTML解析方法不够稳定.

## 浏览结构化数据的方法

~~~python
print(soup.title)
# <ittle>The Dormouse's story</title>

print(soup.title.name)
# u'title'

print(soup.title.string)
# u'The Dormouse's story'

print(soup.p)
# <p class="title"><b>The Dormouse's story</b></p>

print(soup.p['class'])
# u'title'

print(soup.a)
# <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>


print(soup.find(id="link3"))
# <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>

print(soup.find_all('a'))
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
~~~

## 匹配所有a标签的href属性

~~~python
for link in soup.find_all("a"):
	print(link.get("href"))
# http://example.com/elsie
# http://example.com/lacie

# http://example.com/tillie
~~~

## 获取所有文本内容

~~~python
print(soup.get_text())
The Dormouse's story
The Dormouse's story
Once upon a time there were three little sisters; and their names were
            Elsie,
            Lacie and
            Tillie;
            and they lived at the bottom of a well.
...
~~~



### 遍历文档树

​	**以爱丽丝文档为例**

~~~ python
html_doc = """
<html><head><title>The Dormouse's story</title></head>
    <body>
        <p class="title"><b>The Dormouse's story</b></p>
        <p class="story">Once upon a time there were three little sisters; and their names were
            <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
            <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
            <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
            and they lived at the bottom of a well.
        </p>

<p class="story">...</p>
"""
~~~



#### 子节点

> 一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.Beautiful Soup提供了许多操作和遍历子节点的属性.

​	---  操作文档树最简单的方法就是告诉它你想获取的tag的name.如果想获取 <head> 标签

~~~python
soup.head
>>> <head><title>The Dormouse's story</title></head>

soup.title
>>> <title>The Dormouse's story</title>
~~~

> 这是个获取tag的小窍门,可以在文档树的tag中多次调用这个方法.

​	---  下面的代码可以获取<body>标签中的第一个<p>标签:

~~~python
soup.body.p
>>> <p>The Dormouse's story</p>
~~~

> 通过点取属性的方式只能获得当前名字的第一个tag:

~~~python
soup.a
>>> <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
~~~



#### 父节点

> 每个tag或字符串都有父节点:被包含在某个tag中

**parent**

通过 parent 属性来获取某个元素的父节点.在例子“爱丽丝”的文档中,<head>标签是<title>标签的父节点:

~~~ python
title_tag = soup.title
title_tag
# <title>The Dormouse's story</title>

title_tag.parent
# <head><title>The Dormouse's story</title></head>
~~~

#### 兄弟节点

> 看一段代码

~~~python
soup = BeautifulSoup("<a><b>text1</b><c>text2</c></b></a>")
print(soup.prettify())
# <html>
#     <body>
#         <a>
#            <b>
#             text1
#                </b>
#            <c>
#             text2
#                </c>
#         </a>
#     </body>
# </html>
~~~

​	---  因为<b>标签和<c>标签是同一层:他们是同一个元素的子节点,所以<b>和<c>可以被称为兄弟节点.一段文档以标准格式输出时,兄弟节点有相同的缩进级别.在代码中也可以使用这种关系

**next_sibling 和 previous_sibling**

> **在文档树中,使用 next_sibling 和 previous_sibling属性来查询兄弟节点:**

~~~ python
# 下一个兄弟节点
soup.b.next_sibling
>>> <c>text2</c>

# 上一个兄弟节点
soup.c.previous_sibling
>>> <b>text1</b>
~~~



### 搜索文档树

> Beautiful Soup定义了很多搜索方法,这里着重介绍2个: `find()` 和 `find_all()` .其它方法的参数和用法类似,请读者举一反三.

​	**依旧以爱丽丝文档为例**

~~~python
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""
 
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'lxml')
~~~

#### 字符串

> 最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的<b>标签:

~~~python
soup.find_all('b')
>>> [<b>The Dormouse's story</b>]
~~~

#### 列表

> 如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有<a>标签和<b>标签:

~~~python
soup.find_all(["a", "b"])
# [<b>The Dormouse's story</b>,
#  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
~~~

#### 按CSS搜索

> 按照CSS类名搜索tag的功能非常实用,但标识CSS类名的关键字 class在Python中是保留字,使用 class 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 class_ 参数搜索有指定

~~~python
soup.find_all("a", class_="sister")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
~~~

  ---   limit参数

> 文档树中有3个tag符合搜索条件,但结果只返回了2个,因为我们限制了返回数量:

~~~python
soup.find_all("a", limit=2)
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]
~~~

#### css选择器

> Beautiful Soup支持大部分的CSS选择器 ， 在 Tag 或 BeautifulSoup 对象的 .select() 方法中传入字符串参数, 即可使用CSS选择器的语法找到tag:

~~~python
soup.select("title")
# [<title>The Dormouse's story</title>]

soup.select("p:nth-of-type(3)")
# [<p class="story">...</p>]
~~~

##### ---  通过tag标签逐层查找

~~~python
soup.select("body a")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie"  id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

soup.select("html head title")
# [<title>The Dormouse's story</title>]
~~~

##### ---  标签下的直接子标签

~~~python
soup.select("head > title")
# [<title>The Dormouse's story</title>]

soup.select("p > a")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie"  id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

soup.select("p > a:nth-of-type(2)")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

soup.select("p > #link1")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

soup.select("body > a")
# []
~~~

##### --- 通过css类名查找

~~~python
soup.select(".sister")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

soup.select("[class~=sister]")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
~~~

##### --- 通过tag的id查找

~~~python
soup.select("#link1")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

soup.select("a#link2")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]
~~~



## 三、结语

本文的示例放在了我的个人仓库

 [Python爬虫之路](https://github.com/rosyrain/spider-course)  `https://github.com/rosyrain/spider-course` 

lesson10下面，除了上面的示例，还有一个以 `堆糖` 图片网站为例的BeautifulSoup解析获取图片的示例。欢迎各位**Follow/Star/Fork**  ( •̀ ω •́ )✧

通过堆糖的示例，你可以尝试获取 `https://www.fabiaoqing.com/biaoqing` 的前三页图片。


<hr>

​	有任何问题欢迎大家的评论和指正。再次声明，本专栏只做技术探讨，严谨商用，恶意攻击等。

这是我的 GitHub 主页：[Rosyrain (github.com)](https://github.com/Rosyrain)  `https://github.com/rosyrain`，里面有一些我学习时候的笔记或者代码。本专栏的文档和源码存到spider-course的仓库下。

欢迎大家**Follow/Star/Fork**三连。